import itertools, sys, traceback, pathlib, os, time

###############################
###### Preprocess Config ######
###############################

print("Preprocessing config", flush = True)

# Some standard directories
# If you have a lot of data, you can change this to a larger partition.
# Keep in mind though that the $WRKDIR on turso is very fragile and you will likely shoot down the whole cluster if you do not read and understand this first:
# https://wiki.helsinki.fi/display/it4sci/Lustre+User+Guide#LustreUserGuide-4.1FileLockinginLustreisnotanadvisorytobeignored
DATADIR = "data/"
if "datadir" in config:
    DATADIR = config["datadir"]
DATADIR = os.path.abspath(DATADIR)

# If you have separate executables you can store them here.
PROGRAMDIR = "data/program"
if "programdir" in config:
    PROGRAMDIR = config["programdir"]
PROGRAMDIR = os.path.abspath(PROGRAMDIR)

# You can have a separate dir to store the plots/tables generated by your experiments.
REPORTDIR = "data/reports/"
if "reportdir" in config:
    REPORTDIR = config["reportdir"]
REPORTDIR = os.path.abspath(REPORTDIR)

print("DATADIR: {}".format(DATADIR))
print("PROGRAMDIR: {}".format(PROGRAMDIR))
print("REPORTDIR: {}".format(REPORTDIR))

# This is the maximum number of cores for your default ukko2 machine. Vorna machines have 32.
MAX_THREADS = 56
print("Setting MAX_THREADS to " + str(MAX_THREADS), flush = True)

# If you have source code in your repository, you can make snakemake compile it automatically on change by collecting all the source files.
# This is for Rust code, you can apply it to your programming language of choice.
# rust_sources = list(map(str, itertools.chain(pathlib.Path('implementation').glob('**/Cargo.toml'), pathlib.Path('implementation').glob('**/*.rs'))))

# In case you want to access the current date.
import datetime
today = datetime.date.today().isoformat()

# Chosing the number of CPUs used while running
build_cpus = 1

print("Finished config preprocessing", flush = True)

# Always use conda
workflow.use_conda = True

############################
###### File Constants ######
############################

# I like to have global constants for most involved file patterns to prevent typos.
# I keep single letter abreviations in front of the wildcards to make the resulting paths more readable.
# Helps especially when there are many arguments.
READS_SRA = os.path.join(DATADIR, "reads", "s{species}", "sra", "reads.sra")
READS_FASTA = os.path.join(DATADIR, "reads", "s{species}", "fasta", "reads.fa")
ASSEMBLY_SOURCE_READS = os.path.join(DATADIR, "assembly", "s{species}-k{k}", "reads.fa")
ASSEMBLY = os.path.join(DATADIR, "assembly", "s{species}-k{k}", "assembly.fa")
REPORT = os.path.join(REPORTDIR, "s{species}-k{k}", "report.txt")
GENOME_ALL_REFERENCES = os.path.join(DATADIR, "{file_name}.fasta")
GENOME_CIRCULAR_REFERENCES = os.path.join(REPORTDIR, "circularization", "{file_name}_k{k}ma{min_abundance}t{threads}", "report.fasta")
BUILD_FA = os.path.join(REPORTDIR, "bcalm2", "{file_name}_k{k}ma{min_abundance}t{threads}", "report.fasta")
BUILD_LOG = os.path.join("logs", "build_{file_name}_k{k}ma{min_abundance}t{threads}", "log.log")
NODE_TO_ARC_CENTRIC_DBG_BINARY = os.path.abspath("external_software/node-to-arc-centric-dbg/target/release/node-to-arc-centric-dbg")
NODE_TO_ARC_CENTRIC_DBG = os.path.join(REPORTDIR, "node_to_arc", "{file_name}_k{k}ma{min_abundance}t{threads}", "report.edgelist") #threads 28
SAFE_PATHS_BINARY = os.path.abspath("external_software/safe-paths/target/release/flow_decomposition")
SAFE_PATHS = os.path.join(REPORTDIR, "safe_paths", "{file_name}_k{k}ma{min_abundance}t{threads}", "report.fasta") #threads 28
EXTERNAL_SOFTWARE_ROOTDIR = os.path.join(DATADIR, "external_software")
QUAST_BINARY = os.path.join(EXTERNAL_SOFTWARE_ROOTDIR, "quast", "quast.py")
QUAST_OUTPUT_DIR = os.path.join(REPORTDIR, "quast", "{file_name}_k{k}ma{min_abundance}t{threads}")
#     DATADIR = ... # wherever you have your data, e.g. /wrk-vakka/users/<your username>/flowtigs

#################################
###### Global report rules ######
#################################

# By default, snakemake executes the first rule if no target is given.
# If you don't like that, this rule makes it do nothing if no target is given.
localrules: do_nothing
rule do_nothing:
    shell:  "echo 'No target specified'"

# Here can be rules that specify a set of reports you would like to create, like e.g. all executions for all combinations of different k values and species.
localrules: report_all
rule report_all:
    input:  reports = expand(REPORT, species = ["ecoli", "scerevisiae"], k = [23, 31, 39]),

###############################
###### Report Generation ######
###############################

# Here can be rules to generate reports from your experiments, like e.g. plots or tables.
localrules: create_single_report
rule create_single_report:
    input:  assembly = ASSEMBLY,
    output: report = REPORT,
    conda:  "config/conda-biopython-env.yml"
    script: "scripts/create_single_report.py"

########################
###### Assembly ########
########################


# Rule to circularize non-circular sequences of the genome or metagenome.
# input: data of the non-circular sequences of the genome or metagenome in fasta format.
# wildcards: k=size of kmers, file_name=name of the file with the data in the data folder,
#   min_abundance=minimum abundance, threds=number of cpu cores used.
# output: data of the circular sequences of the genome or metagenome in fasta format.
localrules: circularization
rule circularization:
    input:  assembly = GENOME_ALL_REFERENCES,
    output: report = GENOME_CIRCULAR_REFERENCES,
    log:    log = "logs/circularization/{file_name}_k{k}ma{min_abundance}t{threads}/log.log",
    conda:  "config/conda-biopython-env.yml"
    script: "scripts/circularization.py"


# Rule to create a node node-cenric weighted De Bruijn graph using the bcalm2 tool.
# input: data of the circular sequences of the genome or metagenome in fasta format.
# wildcards: k=size of kmers, file_name=name of the file with the data in the data folder,
#   min_abundance=minimum abundance, threds=number of cpu cores used.
# output: Node-centric weighted De Bruijn graph with k-mers of size k in fasta format.
rule bcalm2_build:
    input:  references = GENOME_CIRCULAR_REFERENCES,
    output: tigs = BUILD_FA,
    log:    log = "logs/bcalm2/{file_name}_k{k}ma{min_abundance}t{threads}/log.log",
    params: references = lambda wildcards, input: "'" + "' '".join(input.references) + "'",
    conda:  "config/conda-bcalm2-env.yml",
    threads: MAX_THREADS,
    shadow: "minimal"
    resources:
            mem_mb = 250_000, # probably much more than needed
            time_min = 1440,
            cpus = build_cpus,
            queue = 'short', # I had some more complex expression here, the queues fitting to the time are on https://wiki.helsinki.fi/display/it4sci/HPC+Environment+User+Guide#HPCEnvironmentUserGuide-4.8.4Partitions-Ukko
    shell:  """
        rm -f '{log.log}'
        ${{CONDA_PREFIX}}/bin/time -v bcalm -nb-cores {threads} -kmer-size {wildcards.k} -in '{input.references}' -out '{output.tigs}' -abundance-min {wildcards.min_abundance} 2>&1 | tee -a '{log.log}'
        mv '{output.tigs}.unitigs.fa' '{output.tigs}'
        """


# Rule to set up the external software for the node_to_arc_centric_dbg rule.
rule build_node_to_arc_centric_dbg:
    input:  "external_software/node-to-arc-centric-dbg/Cargo.toml",
    output: NODE_TO_ARC_CENTRIC_DBG_BINARY,
    conda:  "config/conda-rust-env.yml",
    threads: MAX_THREADS,
    resources:
            mem_mb = 10000,
            time_min = 60,
            cpus = MAX_THREADS,
            queue = "aurinko,bigmem,short,medium",
    shell:  """
        cd external_software/node-to-arc-centric-dbg
        cargo build --release -j {threads} --offline
    """


# Rule to transform the node-centric weighted De Bruijn graph given by the bcalm2_build
#   rule to an arc-centric weighted De Bruijn graph.
# input: output of bcalm2_build and output of build_node_to_arc_centric_dbg.
# wildcards: k=size of kmers, file_name=name of the file with the data in the data folder,
#   min_abundance=minimum abundance, threds=number of cpu cores used.
# output: Arc-centric weighted De Bruijn graph with k-mers of size k in fasta format.
rule node_to_arc_centric_dbg:
    input:  node_centric_dbg = BUILD_FA,
            binary = NODE_TO_ARC_CENTRIC_DBG_BINARY,
    log:    log = "logs/node_to_arc/{file_name}_k{k}ma{min_abundance}t{threads}/log.log",
    output: arc_centric_dbg = NODE_TO_ARC_CENTRIC_DBG,   # .edgelist
    conda:  "config/conda-time-env.yml",
    resources:
            time_min = 1440, # likely too much
            mem_mb = 250_000, # likely too much
            queue = "short,medium,bigmem,aurinko",
    shell:  """
        rm -f '{log.log}'
        ${{CONDA_PREFIX}}/bin/time -v '{input.binary}' -k {wildcards.k} --input '{input.node_centric_dbg}' --output '{output.arc_centric_dbg}'
    """




# Rule to set up the external software for the safe-paths rule.
rule build_safe_paths:
    input:  "external_software/safe-paths/Cargo.toml",
    output: SAFE_PATHS_BINARY,
    conda:  "config/conda-rust-env.yml",
    threads: MAX_THREADS,
    resources:
            mem_mb = 10000,
            time_min = 60,
            cpus = MAX_THREADS,
            queue = "aurinko,bigmem,short,medium",
    shell:  """
        cd external_software/safe-paths
        cargo build --release -j {threads} --offline
    """


# Rule to get the safe paths from the edge-centric weighted De Bruijn graph given by
#   the node_to_arc_centric_dbg rule.
# input: output of node_to_arc_centric_dbg and output of build_safe_paths.
# wildcards: k=size of kmers, file_name=name of the file with the data in the data folder,
#   min_abundance=minimum abundance, thredas=number of cpu cores used.
# output: List of the safe paths with, for each path, a line with "Path: <path-index>"
#   and a second line with the sequence.
rule safe_paths:
    input:  arc_centric_dbg = NODE_TO_ARC_CENTRIC_DBG,
            binary = SAFE_PATHS_BINARY,
    log:    log = "logs/safe_paths/{file_name}_k{k}ma{min_abundance}t{threads}/log.log",
    output: safe_paths = SAFE_PATHS,
    conda:  "config/conda-time-env.yml",
    resources:
            time_min = 1440, # likely too much
            mem_mb = 250_000, # likely too much
            queue = "short,medium,bigmem,aurinko",
    shell:  """
        rm -f '{log.log}'
        ${{CONDA_PREFIX}}/bin/time -v '{input.binary}' -k {wildcards.k} --input '{input.arc_centric_dbg}' --output '{output.safe_paths}'
    """





rule run_quast:
    input:  contigs = SAFE_PATHS,
            references = [GENOME_ALL_REFERENCES], # list of references
            script = QUAST_BINARY,
            error_report = "config/quast_error_report.tex",
            error_misassemblies_report = "config/quast_error_misassemblies_report.tex"
    output: directory = directory(QUAST_OUTPUT_DIR),
            eaxmax_csv = os.path.join(QUAST_OUTPUT_DIR, "aligned_stats/EAxmax_plot.csv"),
    log:    minimap = os.path.join(QUAST_OUTPUT_DIR, "contigs_reports", "contigs_report_contigs.stderr")
    params: references = lambda wildcards, input: "-r '" + "' -r '".join(input.references) + "'",
    conda: "config/conda-quast-env.yml"
    threads: 14,
    resources: mem_mb = 250_000, # likely to much for our genomes
               cpus = 14,
               time_min = 1440,
               queue = "short,medium,bigmem,aurinko", # I had some more complex expression here, the queues fitting to the time are on https://wiki.helsinki.fi/display/it4sci/HPC+Environment+User+Guide#HPCEnvironmentUserGuide-4.8.4Partitions-Ukko
    shell:  """
        set +e 
        ${{CONDA_PREFIX}}/bin/time -v {input.script} -t {threads} --no-html -o '{output.directory}' {params.references} '{input.contigs}'
        set -e
        
        if [ $? -ne 0 ]; then
            rm -rf '{output.directory}'
        fi


        mkdir -p '{output.directory}'
        if [ ! -f '{output.directory}/report.tex' ]; then
            echo "report.tex is missing, using error template" 
            cp '{input.error_report}' '{output.directory}/report.tex'
        fi

        mkdir -p '{output.directory}/contigs_reports'
        if [ ! -f '{output.directory}/contigs_reports/misassemblies_report.tex' ]; then
            echo "misassemblies_report.tex is missing, using error template"
            cp '{input.error_misassemblies_report}' '{output.directory}/contigs_reports/misassemblies_report.tex'
        fi

        mkdir -p '{output.directory}/aligned_stats'
        if [ ! -f '{output.directory}/aligned_stats/EAxmax_plot.csv' ]; then
            echo "EAxmax_plot.csv is missing, using empty file"
            touch '{output.directory}/aligned_stats/EAxmax_plot.csv'
        fi
    """ #  --large



#######################
###### Downloads ######
#######################

# Here can be rules to download your input files.
# These must be localrules, as the compute nodes of the federated cluster do not have a connection to the internet.

def get_reads_url(wildcards):
    # Snakemake just swallows whatever errors happen in functions.
    # The only way around that is to handle and print the error ourselves.
    try:
        species = wildcards.species

        if species == "ecoli":
            return "https://sra-downloadb.be-md.ncbi.nlm.nih.gov/sos3/sra-pub-run-20/SRR12793243/SRR12793243.1"
        elif species == "scerevisiae":
            return "https://sra-download.ncbi.nlm.nih.gov/traces/sra78/SRR/014398/SRR14744387"
        else:
            raise Exception("Unknown species: {}".format(species))
    except Exception:
        traceback.print_exc()
        sys.exit("Catched exception")


# Rule to download the external software used for turning the node-centric De Bruijn graph
#   to an arc-centric one.
localrules: download_node_to_arc_centric_dbg
rule download_node_to_arc_centric_dbg:
    output: "external_software/node-to-arc-centric-dbg/Cargo.toml"
    conda:  "config/conda-rust-env.yml"
    threads: 1
    shell:  """
        mkdir -p external_software
        cd external_software

        rm -rf node-to-arc-centric-dbg
        git clone https://github.com/sebschmi/node-to-arc-centric-dbg.git
        cd node-to-arc-centric-dbg
        git checkout d0afdf0532fcaa658f57182fe4e5d26224136285

        cargo fetch
    """


# Rule to download the external software used for calculatings safe paths from an arc-centric De Bruijn graph.
localrules: download_safe_paths
rule download_safe_paths:
    output: "external_software/safe-paths/Cargo.toml"
    conda:  "config/conda-rust-env.yml"
    threads: 1
    shell:  """
        mkdir -p external_software
        cd external_software

        rm -rf safe-paths
        git clone https://github.com/elieling/safe-paths.git
        cd safe-paths
        git checkout db71ee631c4a2f5679ea1ecb06052e233a987e26 

        cargo fetch
    """ #  45588f4986a75254f45cc57e609e518459be937d b0a41fb3b77cf4fab3fcbc07105343543beff917 a2efc2045282c735c182cf15b0f6cbf867c9b757


localrules: install_quast
rule install_quast:
    output: script = QUAST_BINARY,
    params: external_software_dir = EXTERNAL_SOFTWARE_ROOTDIR,
    threads: 1
    shell: """
        mkdir -p '{params.external_software_dir}'
        cd '{params.external_software_dir}'

        rm -rf quast
        git clone https://github.com/elieling/quast.git 
        cd quast
        git checkout 9e374a20bbb058f11bf59fc748abd0e94084d175
    """ # https://github.com/sebschmi/quast  39cf5f04bd0d127480344d5d1c41226245f08b9b




localrules: download_sra_file
rule download_sra_file:
    output: file = READS_SRA,
    params: url = get_reads_url,
    shell:  """
        wget --progress=dot:mega -O '{output.file}' '{params.url}'
    """

# If no resources are specified, the rule will use the default resources specified in config/turso/config.yml
rule convert_sra_download:
    input:  file = READS_SRA,
    output: file = READS_FASTA,
    conda:  "config/conda-convert-reads-env.yml"
    shell:  "fastq-dump --stdout --fasta default '{input.file}' > '{output.file}'"

# I link the reads to each assembly directory separately, such that bcalm creates its auxiliary files in that directory instead of the download directory.
localrules: download_reads
rule download_reads:
    input:  file = READS_FASTA,
    output: file = ASSEMBLY_SOURCE_READS,
    shell:  "ln -sr -T '{input.file}' '{output.file}'"

##############################
###### Download results ######
##############################

# Here is a template for a rule to download report files from turso to your local machine.
# You can use something similar, just make sure to update the paths and the include specification.
#localrules: sync_turso_results
#rule sync_turso_results:
#    conda: "config/conda-rsync-env.yml"
#    shell: """
#        mkdir -p data/reports
#        rsync --verbose --recursive --no-relative --include="*/" --include="report.pdf" --include="aggregated-report.pdf" --exclude="*" turso:'/proj/sebschmi/git/practical-omnitigs/data/reports/' data/reports
#        """

